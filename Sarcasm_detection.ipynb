{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_detection.ipynb",
      "provenance": [],
      "mount_file_id": "1CoBeRv_ireCgUi8pd6PQWwMmHWLGN57m",
      "authorship_tag": "ABX9TyN4iT6fC0P61BBE5ahX9P0T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnusreeChittineni/VAIL_2021/blob/main/Sarcasm_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJtrdzWPdpGi"
      },
      "source": [
        "# unzips neccessarry zip files in Drive\r\n",
        "# only needs to be run once for your Drive\r\n",
        "!unzip -uq \"/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json.zip\" -d \"/content/drive/My Drive/SarcasmDetection\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeA9oVy3el6m",
        "outputId": "3aebe8cc-19ec-4345-aec8-ee5bb7e6ece6"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "# reading data\r\n",
        "def parse_data(file):\r\n",
        "    for l in open(file,'r'):\r\n",
        "        yield json.loads(l)\r\n",
        "\r\n",
        "# intializes a list called data to store data\r\n",
        "data = list(parse_data('/content/drive/MyDrive/SarcasmDetection/Sarcasm_Headlines_Dataset.json'))\r\n",
        "print(data[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUs9vEZak3vL",
        "outputId": "228de878-590f-4934-c600-d20db45bde99"
      },
      "source": [
        "# imports pandas database\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# intializes pandas dataframe to store data\r\n",
        "df = pd.DataFrame(data)\r\n",
        "print(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        article_link  ... is_sarcastic\n",
            "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
            "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
            "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
            "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
            "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZSBAwDU8f5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "import sklearn\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsJ18F1Tfc4-",
        "outputId": "370c6554-a81c-4201-90b1-15e28b3b4509"
      },
      "source": [
        "# initlaizes input and output values\r\n",
        "x = df['headline'].values\r\n",
        "y = df['is_sarcastic'].values\r\n",
        "\r\n",
        "# splits data into training and testing groups\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\r\n",
        "x_test, x_validate, y_test, y_validate = train_test_split(x_test, y_test, test_size=0.50)\r\n",
        "\r\n",
        "# prints shape of data\r\n",
        "print(x_train.shape, x_test.shape, x_validate.shape, y_train.shape, y_test.shape, y_validate.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18696,) (4006,) (4007,) (18696,) (4006,) (4007,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o4_ZlVYU3mO"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(x_train) \r\n",
        "\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "training_sequences = tokenizer.texts_to_sequences(x_train)\r\n",
        "max_sequence_train = max(len(x) for x in training_sequences)\r\n",
        "training_padded = np.array([np.pad(x, (0, max_sequence_train - len(x))) for x in training_sequences])\r\n",
        "\r\n",
        "val_sequences = tokenizer.texts_to_sequences(x_validate)\r\n",
        "max_sequence_val = max(len(x) for x in val_sequences)\r\n",
        "val_padded = np.array([np.pad(x, (0, max_sequence_val - len(x))) for x in val_sequences])\r\n",
        "\r\n",
        "testing_sequences = tokenizer.texts_to_sequences(x_test)\r\n",
        "max_sequence_test = max(len(x) for x in testing_sequences)\r\n",
        "testing_padded = np.array([np.pad(x, (0, max_sequence_test - len(x))) for x in testing_sequences])\r\n",
        "\r\n",
        "#y_test = np.array(y_test)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8dEYC0fU0yn",
        "outputId": "991062f9-e4d6-45a7-b9aa-03f9982c3240"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "        tf.keras.layers.Embedding(len(word_index) + 1, 2, input_shape=[None]),\r\n",
        "        tf.keras.layers.LSTM(2, return_sequences=True),\r\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 2)           49554     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 2)           40        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 1)           3         \n",
            "=================================================================\n",
            "Total params: 49,597\n",
            "Trainable params: 49,597\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgBP_zKJxNzn",
        "outputId": "5f140a8b-a69c-48f7-f709-acc08085e3f2"
      },
      "source": [
        "# try with various optimizer and loss algorithms\r\n",
        "# (SGD, MSE, 56), (Adam, MSE, 93), (Adam, BC, 97)\r\n",
        "model.compile(optimizer='Adam', loss='BinaryCrossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(training_padded, y_train, validation_data=(val_padded, y_validate), \r\n",
        "          epochs=5, verbose=1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "585/585 [==============================] - 12s 18ms/step - loss: 0.6820 - accuracy: 0.5680 - val_loss: 0.4907 - val_accuracy: 0.8147\n",
            "Epoch 2/5\n",
            "585/585 [==============================] - 12s 20ms/step - loss: 0.4283 - accuracy: 0.8536 - val_loss: 0.4192 - val_accuracy: 0.8390\n",
            "Epoch 3/5\n",
            "585/585 [==============================] - 12s 20ms/step - loss: 0.3246 - accuracy: 0.9006 - val_loss: 0.4089 - val_accuracy: 0.8504\n",
            "Epoch 4/5\n",
            "585/585 [==============================] - 12s 20ms/step - loss: 0.2616 - accuracy: 0.9244 - val_loss: 0.4190 - val_accuracy: 0.8500\n",
            "Epoch 5/5\n",
            "585/585 [==============================] - 12s 20ms/step - loss: 0.2163 - accuracy: 0.9404 - val_loss: 0.4474 - val_accuracy: 0.8480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4ffca147f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "7rckRo_igf1o",
        "outputId": "493a19de-600a-44af-fda5-8fa1b9e21dc2"
      },
      "source": [
        "results = model.evaluate(testing_padded, y_test)\r\n",
        "print(\"test loss, test acc:\", results)\r\n",
        "\r\n",
        "predictions = model.predict(testing_padded)\r\n",
        "\r\n",
        "# I don't understand why this line of code does not work\r\n",
        "print(sklearn.metrics.accuracy_score(y_test, predictions, normalize=False))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "126/126 [==============================] - 1s 3ms/step - loss: 0.4434 - accuracy: 0.8367\n",
            "test loss, test acc: [0.4434407949447632, 0.8366615176200867]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-cddfee5960c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
          ]
        }
      ]
    }
  ]
}